{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "RIS_tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp8iYhTsZxt5"
      },
      "source": [
        "# Pruebas RIS\n",
        "### <font color='5E5D5D'> Descripción </font>\n",
        "\n",
        "<i><font color='C9614B'> En esta notebook se encuentran las\n",
        "pruebas realizadas para validar el funcionamiento de la métrica y su utilización (Experimentos 5 y 6.1). Tener en cuenta que la configuración que se presenta aquí, representa únicamente una de las evaluadas.</font></i><br>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpg1ubmKazBA"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5JD9B8Q-i_E"
      },
      "source": [
        "#### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D5YHHAb-i_F"
      },
      "source": [
        "# Librerías del sistema que son prerequsito: descomentar estas líneas únicamente si se necesita su instalación\n",
        "! pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "! apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "# Modelos\n",
        "from models.models import DQNModel, NFQModel\n",
        "\n",
        "# Agentes\n",
        "from agents.dqn_agent import DQNAgent\n",
        "from agents.nfq_agent import NFQAgent\n",
        "\n",
        "# Utilitarios\n",
        "from utils.utils_func import process_state, save_dataset, load_dataset\n",
        "from utils.visualization import plot_results, wrap_env, show_video\n",
        "from utils.metrics import ris_behavior_policy, ris_estimator\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Misc\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sofmax\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isYAVwjw-i_H"
      },
      "source": [
        "## Pruebas para Experimentos 5 y 6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvjkpLAdodBw"
      },
      "source": [
        "La configuración planteada a continuación es la realizada para el experimento 5. Para realizar el expermiento 6 se realizó la configuración mencionada en la documentación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBc9vKzC-i_I"
      },
      "source": [
        "### Deep Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdsDdHAkbJjS"
      },
      "source": [
        "Entrenamiento de agentes DQN, utilizando el ambiente ``MountainCar-v0``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEQwU1p0-i_J"
      },
      "source": [
        "#### Mountain Car"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClU_b-rn-i_K"
      },
      "source": [
        "# Variables globales de entrenamiento\n",
        "BUFFER_SIZE = 2000\n",
        "GAMMA = 0.99\n",
        "NUM_EPISODES = 1000\n",
        "MAX_STEPS = 200\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Generación de datasets (RAND, ESA, EMA)\n",
        "NUM_SAMPLES = 1000\n",
        "NUM_RUNS = 1\n",
        "DATASET_ACTION_TYPE='greedy'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "cqgxcBvv-i_N"
      },
      "source": [
        "# Arrays para los resultados finales\n",
        "rewards_mc_dqn, steps_mc_dqn = [], []\n",
        "agent_mc_dqn = None  \n",
        "\n",
        "# Modelo para agente DQN\n",
        "model_dqn = None\n",
        "\n",
        "# Dataset para guardar las experiencias\n",
        "dataset = []\n",
        "\n",
        "# Lista para almacenar las trayectorias (para el cálculo de RIS)\n",
        "trajectories = []\n",
        "\n",
        "# Seed inicial\n",
        "num_seed = 42\n",
        "\n",
        "for _run in range(NUM_RUNS):\n",
        "  print(f\"\\nRun #{_run+1} | Seed: {num_seed}\")\n",
        "  print(\"********************************************************\")\n",
        "\n",
        "  # Creo el ambiente\n",
        "  env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "  # Seed\n",
        "  env.seed(num_seed)\n",
        "  random.seed(num_seed)\n",
        "  np.random.seed(num_seed)\n",
        "  torch.manual_seed(num_seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  \n",
        "  # Creo el modelo\n",
        "  model_dqn = DQNModel(2, env.action_space.n)\n",
        "\n",
        "  # Creo el agente\n",
        "  agent_mc_dqn = DQNAgent(env, model_dqn, process_state, \n",
        "                          BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, \n",
        "                          epsilon_i=0.99, epsilon_f=0.1, epsilon_anneal_time=1000)\n",
        "\n",
        "  # Entreno al agente\n",
        "  rewards, steps_per_episode = agent_mc_dqn.train(NUM_EPISODES, MAX_STEPS)\n",
        "  \n",
        "  # Resultados\n",
        "  rewards_mc_dqn.append(rewards)\n",
        "  steps_mc_dqn.append(steps_per_episode)\n",
        "\n",
        "  # Genero dataset a partir de la red entrenada\n",
        "  env = gym.make(\"MountainCar-v0\")\n",
        "  experience, traject = agent_mc_dqn.generate_dataset(env, action_type=DATASET_ACTION_TYPE, epsilon=.1, num_samples=NUM_SAMPLES, max_steps=MAX_STEPS)\n",
        "  dataset.extend(experience)\n",
        "  trajectories.extend(traject)\n",
        "\n",
        "  # Incremento el seed\n",
        "  num_seed += 1\n",
        "\n",
        "# Promedio de resultados por corrida\n",
        "rewards_mc_dqn = np.mean(rewards_mc_dqn, axis=0)\n",
        "steps_mc_dqn = np.mean(steps_mc_dqn, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTwsvVn-i_R"
      },
      "source": [
        "# Guardar el dataset de experiencia en formato pickle\n",
        "filename = 'datasets/dataset_sample.pkl'\n",
        "save_dataset(filename, dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE8xU4G8-i_S"
      },
      "source": [
        "# Guardar el dataset de trayectorias en formato pickle\n",
        "filename = 'datasets/trajectories_sample.pkl'\n",
        "save_dataset(filename, trajectories)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCWUIBuw-i_T"
      },
      "source": [
        "### Neural Fitted Q-Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGBcad9OdfnX"
      },
      "source": [
        "Entrenamiento de agentes NFQ, utilizando el ambiente ``MountainCar-v0``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAuJntEX-i_T"
      },
      "source": [
        "#### Mountain Car"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBBcnvj-i_U"
      },
      "source": [
        "# Cargar el dataset de experiencia desde archivo pickle\n",
        "filename = 'datasets/dataset_sample.pkl'\n",
        "dataset = load_dataset(filename)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EmXcRAE-i_U"
      },
      "source": [
        "# Cargar el dataset de trayectorias desde archivo pickle\n",
        "filename = 'datasets/trajectories_sample.pkl'\n",
        "trajectories = load_dataset(filename)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MDjqRe--i_V"
      },
      "source": [
        "# Variables globales de entrenamiento\n",
        "GAMMA = 0.99\n",
        "NUM_EPISODES = 1000\n",
        "MAX_STEPS = 200\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Variables para RIS\n",
        "N_ACTIONS = 3\n",
        "HORIZON = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enkOPqn9-i_V"
      },
      "source": [
        "# Arrays para los resultados finales\n",
        "rewards_mc_nfq, steps_mc_nfq = [], []\n",
        "\n",
        "# Creo el ambiente\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Creo el modelo\n",
        "model_nfq = NFQModel(2, env.action_space.n)\n",
        "\n",
        "# Creo el agente\n",
        "agent_mc_nfq = NFQAgent(env, model_nfq, process_state, \n",
        "                        BATCH_SIZE, LEARNING_RATE, GAMMA,\n",
        "                        dataset, trajectories, \n",
        "                        N_ACTIONS, HORIZON)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "K7NSF1OI-i_W"
      },
      "source": [
        "# Entreno al agente\n",
        "rewards, steps_per_episode, ris_per_episode = agent_mc_nfq.train_from_dataset(NUM_EPISODES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekE-IEg-i_W"
      },
      "source": [
        "# Gráfico de RIS\n",
        "plt.plot(range(NUM_EPISODES), ris_per_episode)\n",
        "plt.title(\"Episode RIS\")\n",
        "plt.xlabel(\"Episode Number\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcMpkPBBGdQa"
      },
      "source": [
        "## Validación del indicador\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cwzIVQ5uSW0"
      },
      "source": [
        "Pruebas para validar el funcionamiento del indicador RIS en diferentes casos según lo mencionado en el experimento 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKnWqN54PL3O"
      },
      "source": [
        "# Cantidad de trayectorias\n",
        "paths_number = len(agent_mc_nfq.trajectories.memory)\n",
        "\n",
        "# Definición de behavior_policy\n",
        "behavior_policy = ris_behavior_policy(agent_mc_nfq.trajectories.memory, agent_mc_nfq.n_actions, paths_number, agent_mc_nfq.horizon)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw_pru5KGghJ"
      },
      "source": [
        "# Resultado RIS\n",
        "ris = ris_estimator(agent_mc_nfq.trajectories.memory,\n",
        "                    agent_mc_dqn.device,\n",
        "                    agent_mc_nfq.policy_net,\n",
        "                    behavior_policy,\n",
        "                    agent_mc_nfq.n_actions,\n",
        "                    agent_mc_nfq.horizon,\n",
        "                    n=None,\n",
        "                    weighted=False)\n",
        "ris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh6dMID9I21L"
      },
      "source": [
        "# Definiciones\n",
        "trajectories_memory = agent_mc_nfq.trajectories.memory\n",
        "PATHS_NUMBERS = len(trajectories_memory)\n",
        "n_actions = 3\n",
        "HORIZON = 1\n",
        "device_ris = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "action_type = DATASET_ACTION_TYPE "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOBR9LALI8eX"
      },
      "source": [
        "# Definiciones de las políticas a utilizar\n",
        "pi_DQN = model_dqn\n",
        "pi_NFQ = model_nfq\n",
        "\n",
        "estimated_pi_DQN = ris_behavior_policy(trajectories_memory,\n",
        "                               N_ACTIONS,\n",
        "                               PATHS_NUMBERS,\n",
        "                               HORIZON)                               "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLe0n32MVdR"
      },
      "source": [
        "# Función implementada para verificar la implementación del indicador\n",
        "def pruebas_ris(trajectories_memory, device_ris, evaluation_policy, paths_numbers, behavior_policy, action_type, horizon):\n",
        "  \n",
        "  # Inicializar los pesos y retorno\n",
        "  ws = np.ones(paths_numbers)\n",
        "  gs = np.zeros(paths_numbers)\n",
        "\n",
        "  # Loop\n",
        "  for x, path in enumerate(trajectories_memory):\n",
        "    long_ob = []\n",
        "\n",
        "    states, actions, rewards, _, _ = list(zip(*path))\n",
        "    gs[x] = sum(rewards)\n",
        "\n",
        "    for state, action in zip(states, actions):\n",
        "      state = tuple(state.numpy())\n",
        "      long_ob.append(state)\n",
        "      state = (torch.tensor(state)).to(device_ris)\n",
        "\n",
        "      # Calcular valor estado-accion para evaluation policy (p) según las distintas opciones\n",
        "      if isinstance(evaluation_policy, dict):\n",
        "        p = evaluation_policy[tuple(long_ob)][action]\n",
        "      elif evaluation_policy == pi_NFQ:\n",
        "        p = evaluation_policy(state).view(-1,1).squeeze().detach()\n",
        "        p = softmax(p)\n",
        "        p = p[action].numpy()\n",
        "      else:\n",
        "        # Evaluar si la transición es un dummy (transiciones agregadas para completar max_steps)\n",
        "        if torch.all(state.eq(torch.tensor([0.55, 0]))).item():\n",
        "          p = 1\n",
        "        else:\n",
        "          p = evaluation_policy(state).view(-1,1).squeeze().detach()\n",
        "          p = softmax(p)\n",
        "          maximum_p = max(p)\n",
        "          p = torch.tensor(list(map(lambda x: 1 if x == maximum_p else 0, p)))\n",
        "          p = p[action].numpy()\n",
        "    \n",
        "      # Calcular valor estado-accion para behavior policy (q) según las distintas opciones\n",
        "      if isinstance(behavior_policy, dict):\n",
        "        q = behavior_policy[tuple(long_ob)][action]\n",
        "      elif behavior_policy == pi_NFQ:\n",
        "        q = behavior_policy(state).view(-1,1).squeeze().detach()\n",
        "        q = softmax(q)\n",
        "        q = q[action].numpy()\n",
        "\n",
        "      else:\n",
        "        # Evaluar si la transición es un dummy (transiciones agregadas para completar max_steps)\n",
        "        if torch.all(state.eq(torch.tensor([0.55, 0]))).item():\n",
        "          q = 1\n",
        "        else:\n",
        "          q = behavior_policy(state).view(-1,1).squeeze().detach()\n",
        "          q = softmax(q)\n",
        "          maximum_q = max(q)\n",
        "          q = torch.tensor(list(map(lambda x: 1 if x == maximum_q else 0, q)))\n",
        "          q = q[action].numpy()\n",
        "        \n",
        "\n",
        "      # Calcular pesos    \n",
        "      ws[x] *= p/q\n",
        "\n",
        "      # Agregar la acción\n",
        "      long_ob.append(action)\n",
        "\n",
        "      # Chequear Horizonte\n",
        "      if len(long_ob) >= horizon * 2:\n",
        "        long_ob = long_ob[2:]\n",
        "\n",
        "  # Calcular valor RIS(n)\n",
        "  ris = np.dot(gs, ws)\n",
        "  ris = ris / paths_numbers\n",
        "\n",
        "  return ris, ws, gs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L-m8MbNNEY0"
      },
      "source": [
        "# Prueba: policy evaluation = pi_NFQ, policy behavior = pi_NFQ\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            pi_NFQ,\n",
        "            PATHS_NUMBERS,\n",
        "            pi_NFQ,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozOlgYMzNL91"
      },
      "source": [
        "# Prueba: policy evaluation = pi_DQN, policy behavior = pi_DQN\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            pi_DQN,\n",
        "            PATHS_NUMBERS,\n",
        "            pi_DQN,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5UBGadINTOR"
      },
      "source": [
        "# Prueba: policy evaluation = estimated_pi_DQN, policy behavior = estimated_pi_DQN\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            estimated_pi_DQN,\n",
        "            PATHS_NUMBERS,\n",
        "            estimated_pi_DQN,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hlnUOMiNljX"
      },
      "source": [
        "# Prueba: policy evaluation = pi_NFQ, policy behavior = pi_DQN\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            pi_NFQ,\n",
        "            PATHS_NUMBERS,\n",
        "            pi_DQN,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkT_r_4Vp1Fa"
      },
      "source": [
        "# Prueba: policy evaluation = pi_DQN, policy behavior = estimated_pi_DQN\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            pi_DQN,\n",
        "            PATHS_NUMBERS,\n",
        "            estimated_pi_DQN,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytYtcYEZp61V"
      },
      "source": [
        "# Prueba: policy evaluation = pi_NFQ, policy behavior = estimated_pi_DQN\n",
        "pruebas_ris(trajectories_memory,\n",
        "            device_ris,\n",
        "            pi_NFQ,\n",
        "            PATHS_NUMBERS,\n",
        "            estimated_pi_DQN,\n",
        "            action_type,\n",
        "            HORIZON)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}